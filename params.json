{"name":"Shuai write up project","tagline":"for class requirement","body":"* title: \"Project_writup\"\r\n* author: \"Shuai Wang\"\r\n* date: \"July 23, 2014\"\r\n\r\nDear peers and graders,\r\nThanks for grading this write_up project! I will describe what I have been doing follwed by the project instruction as well as important procedures learned from class materials. Any of your advice and feedback are valueable for our better understanding of the class lectures.\r\n\r\n## 1 The goal of this project\r\n\r\nAs desribed in the descrption, the goal of this piece of data is to \"predict in which they did the exercise\". \". More specially, the \"classe\" variable that contains \"ABCDE\" five categories is dependent variable. \r\n\r\nAs we learned from week 1, the first thing I did is to look at what the training data looks like. The nature of the data has several volunteers's data on many variables. The snapshot of some variables(you know it is very long!) is shown below:\r\n(https://github.com/ShuaiGitHub/Shuai_Write_up_Project/blob/gh-pages/summary.png)\r\n\r\n### 1.1 In and out of sample errors\r\n\r\nSo by observations, I notice that there are *six* people's data. Since the goal of to predict in which *they* did the excercise, so it is important to conside this factor *before* we train our model. To reduce in-sample errors, we can subset the raw dataset into six dataset with certain names. In other words, we *taliored* the raw data for target people. Actually it greatly reduced out of sample errors in variability of person. Other out of sample errors such as the test case is much smaller so we need to control the virance in order to make the predict model work. Key code is shown below for one example user:\r\n\r\n    *select_subset<- function(DF,DF_test,sub_name)*\r\n\r\n*(DF is traing data frame with sub_name, DF_test is test data frame with sub_name, the predict model will be tailored based on the person)*\r\n\r\ndetails will be given in cleaning procedure since it has to be done in both training and test data set in the same way.\r\n\r\n\r\n### 1.2 Data cleaning\r\n\r\nanother observation from raw data is there are many variables have missing value or nonsense value.\r\n\r\n    *nzv <- nearZeroVar(training_sub); find variables that have near zero variance*\r\n  \r\n    *New_sub <- training_sub[,-nzv]; delete them in training set*\r\n  \r\n    *New_testSub <-test_sub[,-nzv]; delete same columns in test set*\r\n\r\nAfter cleaning, the data summary of one subject was shown below (so many are deleted!):\r\n\r\n(https://github.com/ShuaiGitHub/Shuai_Write_up_Project/blob/gh-pages/Figure/capture.png) \r\n  \r\n## 2 Building models\r\n\r\n### 2.1 cross validation, metric, and model selection\r\nThen the data is sliced by creataDataPartitation in two parts:training and testing for cross validation. Default cross validation such as resampling 25times were also used in train() function.\r\n\r\n### 2.1.1 Machine Learning Algrothim: GLM\r\nActually the first model I try is general linear model since it was learned in week 1(...). The basic idea is simple: after center and scale all remaining variables, fit train(...,method=\"glm\"), when classes is considered as numeric variable.Since the classe is still integer number, So fitted results are rounded to nearest integer by round(). RMSE is used as metric.The result for \"adelmo\" is shown below(using subject \"adelmo\"\" as an example):\r\n\r\n    \"RMSE is 0\"\r\n### 2.1.2 Machine Learning Algrothim:Rpart decision tree\r\nThe \"perfect\" 0 value for RMSE indicate there might be overfitting and something weird. When I apply it to real test, I got large nonsense numbers(-588333 etc) for test data's classe. So it seems that non-linear models are better options. Since the outcome is factor variable, a decision tree was built to predict performance:\r\n\r\n    *modelFit <-train(Subject_train$classe ~.,method=\"rpart\",data=trainPC)*\r\n\r\nResults for one person is shown below(the right panel shows the decision):\r\n\r\n(https://github.com/ShuaiGitHub/Shuai_Write_up_Project/blob/master/Figure/jeremy.png)\r\n\r\nSo I realize there are some factors disturb the whole tree such as \"X\",\"row_names\". After I remove those variables, The rpart tree outputs very reasonable results. My first attempt for the project got 85% correct:\r\n\r\n    *total score: 17/20*\r\n    (only jeremy's prediction has 3 wrong answers, all other get correct prediction.)\r\n\r\n    key code: *New_sub <- New_sub[,!(colnames(New_sub) %in% c(\"cvtd_timestamp\",\"X\",\"row.names\"))]* remove unwanted columns\r\n\r\n### 2.1.3 Machine learning Algrothim: Random Forest\r\n\r\nSince we already know decision tree(rpart) can provide 85% correct. So it is easier for us to try other non-linear algrothim on the test data that got wrong(Jeremy's model). The train function for random forest is shown below:\r\n\r\n    *modelFit <-train(Subject_train$classe ~.,number=3,method=\"rf\",trControl=trainControl(\"cv\"),data=trainPC)*\r\n\r\nThe random forest model is printed on the right(with confusion Matrix). The classe column show the predict value:\r\n\r\n (https://github.com/ShuaiGitHub/Shuai_Write_up_Project/blob/gh-pages/Figure/randomforest.png)\r\n\r\nThis time I got the rest 3 correct.\r\n\r\n    *total score: 20/20*\r\n\r\nThanks for reading it!\r\n\r\nBest,\r\n\r\nShuai\r\n\r\n#### Resources and References:\r\n\r\n[course materials]: slides and examples from class slides\r\n\r\n(http://support.mashery.com/docs/customizing_your_portal/Markdown_Cheat_Sheet)\r\n\r\n(http://www.inside-r.org)\r\n\r\n(http://caret.r-forge.r-project.org)","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}